{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly Download from GC and save as netcdf files\n",
    "- This is for those who cannot use zarr/python for processing the CMIP6 datasets\n",
    "- Please note that the netcdf files have CF-compliant time grids, but might not be what you are used to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gcsfs #google cloud file system. \n",
    "import xarray as xr\n",
    "import warnings\n",
    "from glob import glob # use * !\n",
    "import scipy.io as sio\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utls import search_df, add_time_info, get_zdict #extra functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_area_and_seasonal_mean(danom, xlim, ylim, slim, mask=1):\n",
    "    \"\"\"\n",
    "    Weights each grid point by the cos(latitude), computes area mean, normalizing by areaa mean of the weights\n",
    "    returns:\n",
    "        DataArray:  global mean for each model\n",
    "    \"\"\"  \n",
    "    xlim = np.array(xlim)\n",
    "    xlim += (xlim<0 )*360\n",
    "    if xlim[0]>xlim[1]:\n",
    "        lon_sel = (danom.lon>xlim[0])+(danom.lon<xlim[1])\n",
    "    else:\n",
    "        lon_sel = (danom.lon>xlim[0])*danom.lon<xlim[1]\n",
    "    \n",
    "    if type(mask)!=int:\n",
    "        mask = (xr.ones_like(danom)*mask).isel({'lat': (danom.lat>ylim[0])*(danom.lat<ylim[1]), 'lon': lon_sel})\n",
    "    \n",
    "    danom = danom.isel({'lat': (danom.lat>ylim[0])*(danom.lat<ylim[1]), \n",
    "                        'lon': lon_sel, \n",
    "                        'time': (danom['time'].dt.month >= slim[0])*(danom['time'].dt.month <= slim[1]),\n",
    "                        'plev': danom.plev==100\n",
    "                       })\n",
    "    coslat = np.cos(np.deg2rad(danom.lat))\n",
    "    weights = xr.ones_like(danom)*coslat*mask\n",
    "    weight_mean = weights.mean(['lat','lon'], keep_attrs=True)\n",
    "    area_mean = (danom * weights).mean(['lat','lon'], keep_attrs=True)/weight_mean\n",
    "    if area_mean.time.dtype!='datetime64[ns]':\n",
    "        area_mean['time'] = area_mean.indexes['time'].year\n",
    "        return area_mean.groupby('time').mean(dim='time', keep_attrs=True)\n",
    "        #area_mean.indexes['time'].to_datetimeindex()\n",
    "        #there are lots of dftime.DatetimeNoLeap. this would matter a touch if I used a weighted average.\n",
    "    else:   \n",
    "        #month_length = danom.time.dt.days_in_month\n",
    "        #this actually isn't right! Let's just do an unweighted mean for now :(\n",
    "        #weights = month_length/sum(month_length[slim[0]:slim[1]+1])\n",
    "        #Sm = (weights*area_mean)\n",
    "        #return Sm.groupby(grp).sum(dim='time', keep_attrs=True)\n",
    "        return area_mean.groupby(area_mean.time.dt.year).mean(dim='time', keep_attrs=True)\n",
    "    \n",
    "    #technically this is inconsistent handling; one makes a dimension called \"time\" and the other makes \"years\". I have to deal with that in my matlab code\n",
    "    \n",
    "plev19 = [100000, 92500, 85000, 70000, 60000, 50000, 40000, 30000, 25000, 20000, 15000, 10000, 7000, 5000, 3000, 2000, 1000, 500, 100]\n",
    "plev19_bnds = [100000,95000,90000,75000,65000,55000,45000,35000,27000,23000,17000,12500,8500,6500,4000,2500,1500,700,300,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-853c8c9935e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplev19_bnds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m95000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mplev19_bnds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mplev19_bnds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplev19_bnds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "plev19_bnds = [95000]\n",
    "for i in range(1,17):\n",
    "    plev19_bnds.append(2*ds.plev.values[i] - plev19_bnds[i-1])\n",
    "\n",
    "plev19_bnds\n",
    "2*ds.plev.values[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#danom = ds0\n",
    "\n",
    "#xlim = [-20,40]\n",
    "#ylim = [12,18]\n",
    "#slim = [7,9]\n",
    "def calc_stp(dl):\n",
    "    l = dl.values\n",
    "    stp = max(l[1:] - l[0:-1])\n",
    "    return stp\n",
    "\n",
    "def compute_conv(danom, xlim, ylim, slim, plim):\n",
    "    \"\"\"\n",
    "    Calculates the mean flow convergence of moisture on monthly scale into the Sahel region.\n",
    "    returns convergence at each pressure level. add pressure level ranges to integrate over.\n",
    "    \"\"\"  \n",
    "    xlim = np.array(xlim)\n",
    "    xlim += (xlim<0)*360\n",
    "    lt_stp = calc_stp(danom.lat)\n",
    "    ln_stp = calc_stp(danom.lon)\n",
    "    if xlim[0]>xlim[1]:\n",
    "        lon_sel_2 = (danom.lon>(xlim[0]-ln_stp))+(danom.lon<(xlim[1]+ln_stp))\n",
    "    else:\n",
    "        lon_sel_2 = (danom.lon>(xlim[0]-ln_stp))*(danom.lon<(xlim+ln_stp))\n",
    "\n",
    "    danom = danom.isel({'time': (danom['time'].dt.month >= slim[0])*(danom['time'].dt.month <= slim[1]),\n",
    "                         'lat': (danom.lat>(ylim[0]-lt_stp))*(danom.lat<(ylim[1]+lt_stp)),\n",
    "                         'lon': lon_sel_2,\n",
    "                         'plev': danom.plev==plim #when I can figure out the bounds change this to an integral\n",
    "                       })\n",
    "    danom['qfu'] = danom['hus']*danom['ua']\n",
    "    danom['qfv'] = danom['hus']*danom['va']\n",
    "    if xlim[0]>xlim[1]:\n",
    "        lon_sel = (danom.lon>xlim[0])+(danom.lon<xlim[1])\n",
    "    else:\n",
    "        lon_sel = (danom.lon>xlim[0])*danom.lon<xlim[1]\n",
    "    lat_sel = (danom.lat>ylim[0])*(danom.lat<ylim[1])\n",
    "    danom.load()\n",
    "    dS = danom['qfv'].isel({'lat': danom.lat<(ylim[0]+lt_stp), 'lon': lon_sel}).mean(['lat'], keep_attrs=True)\n",
    "    dN = -danom['qfv'].isel({'lat': danom.lat>(ylim[1]-lt_stp), 'lon': lon_sel}).mean(['lat'], keep_attrs=True)\n",
    "    dW = danom['qfu'].isel({'lat': lat_sel, 'lon': danom.lon<(xlim[0]+ln_stp)}).mean(['lon'], keep_attrs=True)\n",
    "    dE = -danom['qfu'].isel({'lat': lat_sel, 'lon': danom.lon>(xlim[1]-ln_stp)}).mean(['lon'], keep_attrs=True)\n",
    "    coslat = np.cos(np.deg2rad(ylim))\n",
    "    d_conv = (coslat[0]*dS + coslat[1]*dN).sum(['lon'], keep_attrs=True)+(dW+dE).sum(['lat'], keep_attrs=True)\n",
    "    if d_conv.time.dtype!='datetime64[ns]':\n",
    "        d_conv['time'] = d_conv.indexes['time'].year\n",
    "        return d_conv.groupby('time').mean(dim='time', keep_attrs=True)\n",
    "    else:   \n",
    "        return d_conv.groupby(d_conv.time.dt.year).mean(dim='time', keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to write local netcdf files:\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "mach = os.uname()[1]\n",
    "\n",
    "zarr_local = f'/home/{username}/netcdf/cmip6/preprocessed'\n",
    "if not os.path.exists(zarr_local):\n",
    "    print(f'Please create the directory {zarr_local}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>zstore</th>\n",
       "      <th>dcpp_init_year</th>\n",
       "      <th>version</th>\n",
       "      <th>status</th>\n",
       "      <th>severity</th>\n",
       "      <th>issue_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HighResMIP</td>\n",
       "      <td>CMCC</td>\n",
       "      <td>CMCC-CM2-HR4</td>\n",
       "      <td>highresSST-present</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>uas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20170706</td>\n",
       "      <td>good</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HighResMIP</td>\n",
       "      <td>CMCC</td>\n",
       "      <td>CMCC-CM2-HR4</td>\n",
       "      <td>highresSST-present</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>rlus</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20170706</td>\n",
       "      <td>good</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HighResMIP</td>\n",
       "      <td>CMCC</td>\n",
       "      <td>CMCC-CM2-HR4</td>\n",
       "      <td>highresSST-present</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>psl</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20170706</td>\n",
       "      <td>good</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HighResMIP</td>\n",
       "      <td>CMCC</td>\n",
       "      <td>CMCC-CM2-HR4</td>\n",
       "      <td>highresSST-present</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>rlds</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20170706</td>\n",
       "      <td>good</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HighResMIP</td>\n",
       "      <td>CMCC</td>\n",
       "      <td>CMCC-CM2-HR4</td>\n",
       "      <td>highresSST-present</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>ps</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20170706</td>\n",
       "      <td>good</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activity_id institution_id     source_id       experiment_id member_id  \\\n",
       "0  HighResMIP           CMCC  CMCC-CM2-HR4  highresSST-present  r1i1p1f1   \n",
       "1  HighResMIP           CMCC  CMCC-CM2-HR4  highresSST-present  r1i1p1f1   \n",
       "2  HighResMIP           CMCC  CMCC-CM2-HR4  highresSST-present  r1i1p1f1   \n",
       "3  HighResMIP           CMCC  CMCC-CM2-HR4  highresSST-present  r1i1p1f1   \n",
       "4  HighResMIP           CMCC  CMCC-CM2-HR4  highresSST-present  r1i1p1f1   \n",
       "\n",
       "  table_id variable_id grid_label  \\\n",
       "0     Amon         uas         gn   \n",
       "1     Amon        rlus         gn   \n",
       "2     Amon         psl         gn   \n",
       "3     Amon        rlds         gn   \n",
       "4     Amon          ps         gn   \n",
       "\n",
       "                                              zstore dcpp_init_year   version  \\\n",
       "0  gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...            NaN  20170706   \n",
       "1  gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...            NaN  20170706   \n",
       "2  gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...            NaN  20170706   \n",
       "3  gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...            NaN  20170706   \n",
       "4  gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...            NaN  20170706   \n",
       "\n",
       "  status severity issue_url  \n",
       "0   good     none      none  \n",
       "1   good     none      none  \n",
       "2   good     none      none  \n",
       "3   good     none      none  \n",
       "4   good     none      none  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the master CMIP6 Google Cloud catalog\n",
    "df_cloud = pd.read_csv('https://cmip6.storage.googleapis.com/cmip6-zarr-consolidated-stores-noQC.csv', dtype='unicode')\n",
    "df_cloud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose basic configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we search the CMIP6 data for the datasets you need - using the same keywords as at the ESGF sites\n",
    "#       https://esgf-node.llnl.gov/search/cmip6/\n",
    "\n",
    "debug = False\n",
    "\n",
    "# must choose ONE table_id  (only works for *mon or *day)\n",
    "table_id = 'Amon'\n",
    "\n",
    "#must choose LIST of experiments, variables\n",
    "experiments = ['historical']#,'hist-aer', , 'hist-nat', 'hist-GHG', 'piControl' 'amip-hist',\n",
    "variables = ['ta']\n",
    "\n",
    "location = 'Sahel'#'Ocean'#\n",
    "\n",
    "# can specify 'All' or give a list or string\n",
    "sources = ['MPI-ESM1-2-LR']#GFDL-CM4']#'ACCESS-ESM1-5']#'CNRM-ESM2-1']#'NorESM2-LM']# 'GFDL-ESM4'['CanESM5-CanOE']#CMCC-CM2-SR5']#'CIESM']#MCM-UA-1-0']  #AWI-CM-1-1-MR']#SAM0-UNICON']# omit the [] to get all models with CESM2 in their name\n",
    "#sources = 'All'\n",
    "members = ['r6i1p1f1']#r1i1p1f1']#'r9i1p1f2']#\n",
    "#members = 'All'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of matching datasets 1\n"
     ]
    }
   ],
   "source": [
    "search = {'table_id':table_id}\n",
    "search['experiment_id'] = experiments\n",
    "search['variable_id'] = variables\n",
    "if sources != 'All':\n",
    "    search['source_id'] = sources\n",
    "if members != 'All':\n",
    "    search['member_id'] = members\n",
    "    \n",
    "df_available = search_df(df_cloud, **search)\n",
    "\n",
    "print('number of matching datasets',len(df_available))\n",
    "\n",
    "#523 historical simulations < 536 on the cite directly. Do I want to figure out which simulations are missing? \n",
    "# Or do I trust that they are missing for a reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>zstore</th>\n",
       "      <th>dcpp_init_year</th>\n",
       "      <th>version</th>\n",
       "      <th>status</th>\n",
       "      <th>severity</th>\n",
       "      <th>issue_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223742</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>MPI-M</td>\n",
       "      <td>MPI-ESM1-2-LR</td>\n",
       "      <td>historical</td>\n",
       "      <td>r6i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>ta</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/CMIP/MPI-M/MPI-ESM1-2-LR/hist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190710</td>\n",
       "      <td>good</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity_id institution_id      source_id experiment_id member_id  \\\n",
       "223742        CMIP          MPI-M  MPI-ESM1-2-LR    historical  r6i1p1f1   \n",
       "\n",
       "       table_id variable_id grid_label  \\\n",
       "223742     Amon          ta         gn   \n",
       "\n",
       "                                                   zstore dcpp_init_year  \\\n",
       "223742  gs://cmip6/CMIP6/CMIP/MPI-M/MPI-ESM1-2-LR/hist...            NaN   \n",
       "\n",
       "         version status severity issue_url  \n",
       "223742  20190710   good     none      none  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_available.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For proper debugging, it is helpful to add time grid information to dataframe:\n",
    "if debug:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        dfa = add_time_info(df_available)\n",
    "else:\n",
    "    dfa = df_available.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only use the MASK code for TS!\n",
    "if location=='Ocean':\n",
    "    search_mask = {'table_id':'fx'}\n",
    "    search_mask['experiment_id'] = ['historical', 'piControl', '1pctCO2','hist-resIPO','hist-1950HC']\n",
    "    search_mask['variable_id'] = ['sftlf']\n",
    "    if sources != 'All':\n",
    "        search_mask['source_id'] = sources\n",
    "    if members != 'All':\n",
    "        search_mask['member_id'] = members\n",
    "    historical_mask = search_df(df_cloud, **search_mask)\n",
    "\n",
    "    print('number of mask datasets',len(historical_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only use MASK code for TS!\n",
    "if location=='Ocean':\n",
    "    def get_ids(dfa, id_name):\n",
    "        zdicts = list(map(get_zdict, list(dfa.zstore.values)))\n",
    "        return set(map(lambda x: x[id_name], zdicts))\n",
    "\n",
    "    id_name = 'source_id'\n",
    "    historical_mask_models = get_ids(historical_mask, id_name)\n",
    "    sst_models = get_ids(dfa, id_name)\n",
    "\n",
    "    maskable_models = sst_models.intersection(historical_mask_models)\n",
    "\n",
    "    gsurls = np.array([gsurl for gsurl in dfa.zstore.values if get_zdict(gsurl)['source_id'] in maskable_models])\n",
    "    missing = np.array([gsurl for gsurl in dfa.zstore.values if not get_zdict(gsurl)['source_id'] in maskable_models])\n",
    "\n",
    "    masks = historical_mask.groupby('source_id').first()\n",
    "\n",
    "    gsurls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if location=='Ocean':\n",
    "    missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pr:\n",
    "if location=='Sahel':\n",
    "    gsurls = np.array(dfa.zstore.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    dm = dfa[['experiment_id','source_id','member_id','variable_id','start','stop']].groupby([\n",
    "             'experiment_id','start','stop','source_id']).nunique()[['member_id']]\n",
    "\n",
    "    table = pd.DataFrame.pivot_table(dm,\n",
    "                                     values='member_id',\n",
    "                                     index=['source_id','start','stop'],\n",
    "                                     columns=['experiment_id'],\n",
    "                                     aggfunc=np.sum,\n",
    "                                     fill_value=0)\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa.issue_url.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only') #the actual files, not the list of files woohoo FILE SYSTEM\n",
    "#fs.get_mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: www.googleapis.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving file /home/rebecca/netcdf/cmip6/preprocessed/historical/ta_MPI-M_MPI-ESM1-2-LR_r6i1p1f1.nc\n"
     ]
    }
   ],
   "source": [
    "#gsurls = dfa.zstore.values #zstore is the url where the data is stored\n",
    "\n",
    "conv = False\n",
    "replace = True\n",
    "\n",
    "ds_list = []\n",
    "ds_failed_list = []\n",
    "\n",
    "def update_vars(ds):\n",
    "    for var in [var for var in ds.coords]:\n",
    "        if 'bounds' in var:\n",
    "            nvar = var.replace('bounds','bnds')\n",
    "            #print(var,nvar)\n",
    "            ds = ds.rename({var:nvar})\n",
    "        if 'latitude' in var:\n",
    "            nvar = var.replace('latitude','lat')\n",
    "            #print(var,nvar)\n",
    "            ds = ds.rename({var:nvar})\n",
    "        if 'longitude' in var:\n",
    "            nvar = var.replace('longitude','lon')\n",
    "            #print(var,nvar)\n",
    "            ds = ds.rename({var:nvar})\n",
    "    return ds\n",
    "\n",
    "if conv:\n",
    "    search_uv = search.copy()\n",
    "    search_uv['source_id'] = [model]\n",
    "    search_uv['member_id'] = [run]\n",
    "    search_u = search_uv.copy()\n",
    "    search_v = search_uv.copy()\n",
    "    search_u['variable_id'] = ['ua']\n",
    "    search_v['variable_id'] = ['va']\n",
    "    \n",
    "for gsurl in gsurls:\n",
    "    zdict = get_zdict(gsurl) #naomi func for metadata\n",
    "    institution = zdict['institution_id']\n",
    "    model = zdict['source_id']\n",
    "    run = zdict['member_id']\n",
    "    variable = zdict['variable_id']\n",
    "    expt = zdict['experiment_id']\n",
    "    if conv:\n",
    "        filename = f'{variable}_conv_925_{institution}_{model}_{run}'\n",
    "    else:\n",
    "        filename = f'{variable}_{institution}_{model}_{run}'\n",
    "    ncdir = f'{zarr_local}/{expt}'\n",
    "    ncfile = f'{ncdir}/{filename}.nc'\n",
    "    \n",
    "    if(not replace):\n",
    "        ncfiles = glob(ncfile) #check not to double-download files\n",
    "        if len(ncfiles) > 0:\n",
    "            print(ncfiles, 'already exists')\n",
    "            continue\n",
    "            \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        ds0 = xr.open_zarr(fs.get_mapper(gsurl),consolidated=True) #gets info about the file. get_mapper! always use consolidated=True\n",
    "        if variable=='ts':\n",
    "            mask_ds = xr.open_zarr(fs.get_mapper(masks['zstore'][model]),consolidated=True) \n",
    "        #if variable=='ta' or variable=='zg':\n",
    "         #   ds0 = ds0.isel({'plev': ds0.plev==100})\n",
    "        if conv:\n",
    "            U = np.array(search_df(df_cloud, **search_u).zstore.values)\n",
    "            V = np.array(search_df(df_cloud, **search_v).zstore.values)\n",
    "            if len(U) == 1 and len(V) == 1:\n",
    "                dsU = xr.open_zarr(fs.get_mapper(U[0]),consolidated=True)\n",
    "                dsV = xr.open_zarr(fs.get_mapper(V[0]),consolidated=True)\n",
    "                ds0['va'] = dsV['va']\n",
    "                ds0['ua'] = dsU['ua']\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    #month_length = ds.time.dt.days_in_month #for some reason the first one doesn't have this...\n",
    "        \n",
    "    ds0 = update_vars(ds0)\n",
    "    \n",
    "    if location=='Ocean':\n",
    "        mask_ds = update_vars(mask_ds)\n",
    "        ls_mask = np.floor(1-mask_ds.sftlf.values/100)\n",
    "    \n",
    "    try:\n",
    "        #if variable=='hus':\n",
    "        #    ds = ds0.isel({'plev':ds0.plev==8.5e4})\n",
    "        #else:\n",
    "        ds = ds0\n",
    "        if location=='Ocean':\n",
    "            NA = compute_area_and_seasonal_mean(ds, [-75,-15], [10,40], [7,9])\n",
    "            GT = compute_area_and_seasonal_mean(ds, [0,360],[-20,20],[7,9], mask=ls_mask)\n",
    "            SA = compute_area_and_seasonal_mean(ds, [-30, 20], [-20,10], [7,9], mask=ls_mask)\n",
    "            md = compute_area_and_seasonal_mean(ds, [-6,36], [30,40], [7,9], mask=ls_mask)\n",
    "            TA = compute_area_and_seasonal_mean(ds, [-75,-15], [5,15], [7,9])\n",
    "            Sm = NA\n",
    "            Sm = Sm.rename_vars({'ts':'NA'})\n",
    "            Sm['GT'] = GT.ts\n",
    "            Sm['NARI'] = NA.ts - GT.ts\n",
    "            Sm['SA'] = SA.ts\n",
    "            Sm['md'] = md.ts\n",
    "            Sm['TA'] = TA.ts\n",
    "        elif conv:\n",
    "            Sm = compute_conv(ds, [-20,40], [12,18], [7,9], 92500)\n",
    "        else:\n",
    "            Sm = compute_area_and_seasonal_mean(ds, [-20,40], [12,18], [7,9])\n",
    "  \n",
    "    except ValueError:\n",
    "        print(f'value error for {ncfile}')\n",
    "        ds_failed_list += [ds]\n",
    "        continue\n",
    "        \n",
    "    #have to customize this to the variable I'm using!\n",
    "    if variable=='ts' or variable=='tas' or variable=='ta':\n",
    "        if not ds[variable].attrs['units']=='K':\n",
    "            print(\"cannot comprehend units ({}), skipping model {}\".format(ds[variable].attrs['units'], model))\n",
    "            continue\n",
    "    elif variable=='pr':\n",
    "        if ds.pr.attrs['units'] == 'kg m-2 s-1':\n",
    "            Sm *= 86400\n",
    "        else:\n",
    "            print(\"cannot comprehend units ({}), skipping model {}\".format(ds.pr.attrs['units'], model))\n",
    "            continue\n",
    "    elif variable=='zg':\n",
    "        if not ds[variable].attrs['units']=='m':\n",
    "            print(\"cannot comprehend units ({}), skipping model {}\".format(ds[variable].attrs['units'], model))\n",
    "            continue\n",
    "    elif (variable=='huss'):\n",
    "        Sm *= float(ds.huss.attrs['units'])*1000 #convert to g/kg\n",
    "    elif (variable=='hus'):\n",
    "        Sm *= float(ds.hus.attrs['units'])*1000 #convert to g/kg\n",
    "    else:\n",
    "        print(\"need to make new units case for variable {} with units {}\".format(variable, ds[variable].attrs['units']))\n",
    "\n",
    "    os.system(f'mkdir -p {ncdir}')\n",
    "    try:\n",
    "        Sm.to_netcdf(ncfile,mode='w',unlimited_dims=['time','year'])  #saves the file. Don't have to do this before I'm ready! But ds is replaced each time...\n",
    "        ds_list += [Sm]\n",
    "        print(f'saving file {ncfile}')\n",
    "    except ValueError:\n",
    "        print(f'value error for {ncfile}')\n",
    "        ds_failed_list += [Sm]\n",
    "        continue    \n",
    "        \n",
    "    #ok I got an error for a model which uses i/j coordinates instead of lat lon! OY VEY...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "\n",
    "ds.ts[0].plot()\n",
    "(ds*xr.ones_like(ds)*np.cos(np.deg2rad(ds.lat))*ls_mask).ts[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (plev: 1, year: 165)\n",
       "Coordinates:\n",
       "  * plev     (plev) float64 100.0\n",
       "  * year     (year) int64 1850 1851 1852 1853 1854 ... 2010 2011 2012 2013 2014\n",
       "Data variables:\n",
       "    ta       (year, plev) float64 dask.array<chunksize=(1, 1), meta=np.ndarray>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tree -L 9 ~/CMIP6-downloads #unix tree of created files; I didn't create any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! du -sh ~/CMIP6-downloads/*/*/*/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/home/naomi/CMIP6-downloads/CMIP/NCAR/CESM2/historical/r1i1p1f1/Amon/sfcWind/gn/sfcWind.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sfcWind.plot(vmin=0,vmax=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert longitude coordinates from 0-359 to -180-179:\n",
    "\n",
    "ds2 = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "#or\n",
    "\n",
    "ds.coords['lon'] = (ds.coords['lon'] + 180) % 360 - 180\n",
    "ds = ds.sortby(ds.lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.sfcWind[0].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myPython3.6",
   "language": "python",
   "name": "my3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
